{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb9c45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import polars as pl\n",
    "import io\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eaa24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"GÃ¨re le tÃ©lÃ©chargement des donnÃ©es Binance\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, period: str = \"monthly\",\n",
    "                 target: str = \"aggTrades\", data_dir: str = \"./data\"):\n",
    "        self.base_url = base_url\n",
    "        self.period = period\n",
    "        self.target = target\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Fichier JSON pour tracker les tÃ©lÃ©chargements\n",
    "        self.tracker_file = self.data_dir / \"downloaded_files.json\"\n",
    "        self.downloaded = self._load_tracker()\n",
    "    \n",
    "    def _load_tracker(self) -> dict:\n",
    "        \"\"\"Charge le fichier JSON de tracking\"\"\"\n",
    "        if self.tracker_file.exists():\n",
    "            with open(self.tracker_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \"\"\"\n",
    "    def _save_tracker(self):\n",
    "        with open(self.tracker_file, 'a') as f:\n",
    "            json.dump(self.downloaded, f, indent=2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _save_tracker(self):\n",
    "        \"\"\"Sauvegarde le fichier JSON de tracking\"\"\"\n",
    "        # Charger les donnÃ©es existantes\n",
    "        if os.path.exists(self.tracker_file):\n",
    "            with open(self.tracker_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        else:\n",
    "            data = {}\n",
    "        \n",
    "        # Fusionner avec les nouvelles donnÃ©es\n",
    "        data.update(self.downloaded)\n",
    "        \n",
    "        # Sauvegarder (mode 'w' pour Ã©craser avec la version fusionnÃ©e)\n",
    "        with open(self.tracker_file, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    def _mark_downloaded(self, symbol: str, date: datetime):\n",
    "        \"\"\"Marque un fichier comme tÃ©lÃ©chargÃ©\"\"\"\n",
    "        key = f\"{symbol}_{date.year}-{date.month:02d}\"\n",
    "        self.downloaded[key] = {\n",
    "            'symbol': symbol,\n",
    "            'year': date.year,\n",
    "            'month': date.month,\n",
    "            'date' : date.day,\n",
    "            'period': self.period,\n",
    "            'downloaded_at': datetime.now().isoformat()\n",
    "        }\n",
    "        self._save_tracker()\n",
    "    \n",
    "    def _is_downloaded(self, symbol: str, date: datetime) -> bool:\n",
    "        \"\"\"VÃ©rifie si un fichier a dÃ©jÃ  Ã©tÃ© tÃ©lÃ©chargÃ©\"\"\"\n",
    "        key = f\"{symbol}_{date.year}-{date.month:02d}\"\n",
    "        return key in self.downloaded\n",
    "    \n",
    "    def _get_filename(self, symbol: str, date: datetime) -> str:\n",
    "        \"\"\"Construit le nom de fichier selon le period\"\"\"\n",
    "        if self.period == \"monthly\":\n",
    "            return f'{symbol}-{self.target}-{date.year}-{date.month:02d}.csv'\n",
    "        elif self.period == \"daily\":\n",
    "            return f'{symbol}-{self.target}-{date.year}-{date.month:02d}-{date.day:02d}.csv'\n",
    "        else:\n",
    "            raise ValueError(f\"Period invalide: {self.period}\")\n",
    "    \n",
    "    def _get_url(self, symbol: str, date: datetime) -> str:\n",
    "        \"\"\"Construit l'URL de tÃ©lÃ©chargement\"\"\"\n",
    "        filename_base = self._get_filename(symbol, date).replace('.csv', '.zip')\n",
    "        return f\"{self.base_url}/{self.period}/{self.target}/{symbol}/{filename_base}\"\n",
    "    \n",
    "    def _download(self, url: str) -> bytes | None:\n",
    "        \"\"\"TÃ©lÃ©charge un fichier ZIP\"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            return r.content if r.status_code == 200 else None\n",
    "        except (requests.RequestException, TimeoutError):\n",
    "            return None\n",
    "    \n",
    "    def _unzip(self, content: bytes, symbol: str, date: datetime) -> Path | None:\n",
    "        \"\"\"DÃ©compresse un fichier ZIP et retourne le chemin du CSV\"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(io.BytesIO(content)) as z:\n",
    "                z.extractall(self.data_dir)\n",
    "            \n",
    "            csv_file = self.data_dir / self._get_filename(symbol, date)\n",
    "            return csv_file if csv_file.exists() else None\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur dÃ©compression: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load(self, symbol: str, date: datetime) -> Path | None:\n",
    "        \"\"\"Charge un fichier (check tracker â†’ check file â†’ download â†’ unzip)\"\"\"\n",
    "        \n",
    "        # 1. Check si dÃ©jÃ  tÃ©lÃ©chargÃ© selon le tracker\n",
    "        if self._is_downloaded(symbol, date):\n",
    "            # VÃ©rifier quand mÃªme si le fichier physique existe\n",
    "            csv_file = self.data_dir / self._get_filename(symbol, date)\n",
    "            if csv_file.exists():\n",
    "                return csv_file\n",
    "            # Si le fichier n'existe plus mais est dans le tracker, on continue le tÃ©lÃ©chargement\n",
    "        \n",
    "        # 2. Download\n",
    "        url = self._get_url(symbol, date)\n",
    "        content = self._download(url)\n",
    "        if not content:\n",
    "            return None\n",
    "        \n",
    "        # 3. Unzip\n",
    "        csv_file = self._unzip(content, symbol, date)\n",
    "        \n",
    "        # 4. Marquer comme tÃ©lÃ©chargÃ©\n",
    "        if csv_file:\n",
    "            self._mark_downloaded(symbol, date)\n",
    "        \n",
    "        return csv_file\n",
    "    \n",
    "    \n",
    "\n",
    "class BarMethods:\n",
    "    \n",
    "    def __init__(self, df: pl.DataFrame,\n",
    "                 bar_params: dict = {'type': 'time', 'params': '5m'}):\n",
    "        self.df_raw = df\n",
    "        self.bar_params = bar_params\n",
    "        self._time_bar()\n",
    "\n",
    "    def _time_bar(self):\n",
    "        self.df_agg = (\n",
    "            self.df_raw\n",
    "            .group_by_dynamic('timestamp', every=self.bar_params.get('params', '5m'))\n",
    "            .agg([\n",
    "                pl.col('price').first().alias('open'),\n",
    "                pl.col('price').max().alias('high'),\n",
    "                pl.col('price').min().alias('low'),\n",
    "                pl.col('price').last().alias('close'),\n",
    "                pl.col('quantity').sum().alias('volume'),\n",
    "                pl.col('buy_volume').sum().alias('buy_volume'),\n",
    "                pl.col('sell_volume').sum().alias('sell_volume'),\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('buy_volume') - pl.col('sell_volume')).alias('delta'),\n",
    "                (pl.col('buy_volume') + pl.col('sell_volume')).alias('total_volume')\n",
    "            ])\n",
    "            .with_columns([\n",
    "                (pl.col('delta') / pl.col('total_volume')).alias('imbalance'),\n",
    "                pl.col('delta').cum_sum().alias('cvd')\n",
    "            ])\n",
    "        )\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Traite, agrÃ¨ge et sauvegarde les donnÃ©es\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str,\n",
    "                 period: str,\n",
    "                 bar_params: dict = {'type': 'time', 'params': '5m'}):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.period = period\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self.bar_params = bar_params\n",
    "    \n",
    "    def _preprocess(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        \"\"\"PrÃ©pare les donnÃ©es brutes\"\"\"\n",
    "        return df.with_columns([\n",
    "            pl.from_epoch(pl.col('transact_time'), time_unit='ms').alias('timestamp'),\n",
    "            pl.col('price').cast(pl.Float64),\n",
    "            pl.col('quantity').cast(pl.Float64),\n",
    "            pl.when(~pl.col('is_buyer_maker')).then(pl.col('quantity')).otherwise(0).alias('buy_volume'),\n",
    "            pl.when(pl.col('is_buyer_maker')).then(pl.col('quantity')).otherwise(0).alias('sell_volume'),\n",
    "            (pl.col('price') * pl.col('quantity')).alias('dollar_volume')\n",
    "        ]).with_columns([\n",
    "            pl.col('quantity').cum_sum().alias('cumulative_volume'),\n",
    "            pl.col('dollar_volume').cum_sum().alias('cumulative_dollar')\n",
    "        ])\n",
    "    \n",
    "    def _clear(self, filepath: Path, *objects):\n",
    "        \"\"\"LibÃ¨re la mÃ©moire et supprime le CSV\"\"\"\n",
    "        # Supprimer le CSV source\n",
    "        if filepath.exists():\n",
    "            filepath.unlink()\n",
    "        \n",
    "        # LibÃ©rer les objets\n",
    "        for obj in objects:\n",
    "            del obj\n",
    "        gc.collect()\n",
    "        \n",
    "    \n",
    "    def execute(self, filepath: Path, symbol: str, date: datetime):\n",
    "        \"\"\"Pipeline complet: read â†’ preprocess â†’ aggregate â†’ save â†’ clear\"\"\"\n",
    "        \n",
    "        # Check header\n",
    "        with open(filepath, 'r') as f:\n",
    "            first_line = f.readline()\n",
    "        has_header = 'agg_trade_id' in first_line or 'price' in first_line\n",
    "        \n",
    "        # Read\n",
    "        if has_header:\n",
    "            df = pl.read_csv(filepath, has_header=True)\n",
    "        else:\n",
    "            df = pl.read_csv(filepath,\n",
    "                         has_header=False,\n",
    "                         new_columns=['agg_trade_id', 'price', 'quantity',\n",
    "                                      'first_trade_id', 'last_trade_id',\n",
    "                                      'transact_time', 'is_buyer_maker']\n",
    "                         )\n",
    "        \n",
    "        # Preprocess\n",
    "        df_prep = self._preprocess(df)\n",
    "        \n",
    "        # Aggregate\n",
    "        bar_methods = BarMethods(df_prep, bar_params=self.bar_params)\n",
    "        df_agg = bar_methods.df_agg\n",
    "        \n",
    "        # Save\n",
    "        bar_str = self.bar_params.get('params', '5m')\n",
    "        if self.period == \"monthly\":\n",
    "            filename = self.data_dir / f\"{symbol}_{bar_str}_{date.year}-{date.month:02d}.parquet\"\n",
    "            df_agg.write_parquet(filename)\n",
    "            print(f\"â†’ {filename.name} ({len(df_agg)} barres)\")\n",
    "            self._clear(filepath, df, df_prep, df_agg, bar_methods)\n",
    "        \n",
    "        elif self.period == \"daily\":\n",
    "            filename = self.data_dir / f\"{symbol}_{bar_str}_{date.year}-{date.month:02d}-{date.day:02d}.parquet\"\n",
    "            df_agg.write_parquet(filename)\n",
    "            print(f\"â†’ {filename.name} ({len(df_agg)} barres)\")\n",
    "            self._clear(filepath, df, df_prep, df_agg, bar_methods)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def run_one(symbol, date, data_dir, period=\"monthly\", bar_params={'type': 'time', 'params': '5m'}):\n",
    "    loader = DataLoader(base_url=\"https://data.binance.vision/data/futures/um\", \n",
    "                    period=period, target=\"aggTrades\", data_dir=data_dir)\n",
    "    csv_file = loader.load(symbol=symbol, date=date)\n",
    "    if csv_file:    \n",
    "        print(csv_file)\n",
    "        processor = DataProcessor(data_dir=data_dir, period=period, bar_params=bar_params)\n",
    "        processor.execute(filepath=csv_file, symbol=symbol, date=date)\n",
    "    else:\n",
    "        print(\"âœ— Fichier non disponible\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5acec386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def run_one_wrapper(args):\n",
    "    \"\"\"Wrapper pour run_one avec unpacking des arguments\"\"\"\n",
    "    symbol, date, data_dir, period, bar_params = args\n",
    "    try:\n",
    "        run_one(symbol=symbol, date=date, data_dir=data_dir,\n",
    "                period=period, bar_params=bar_params)\n",
    "        return (symbol, date, True)\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Erreur {symbol} {date.year}-{date.month:02d}: {e}\")\n",
    "        return (symbol, date, False)\n",
    "\n",
    "def run_parallel(symbols: List[str], dates: List[datetime], \n",
    "                data_dir: str = \"./data\",\n",
    "                period: str = \"monthly\",\n",
    "                bar_params: dict = {'type': 'time', 'params': '5m'},\n",
    "                max_workers: int = 4):\n",
    "    \n",
    "    # CrÃ©er toutes les combinaisons (symbol, date)\n",
    "    tasks = [(symbol, date, data_dir, period, bar_params) for symbol in symbols for date in dates]\n",
    "    print(f\"ðŸš€ DÃ©marrage: {len(tasks)} tÃ¢ches avec {max_workers} workers\\n\")\n",
    "    \n",
    "    completed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(run_one_wrapper, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            symbol, date, success = future.result()\n",
    "            completed += 1\n",
    "            if not success:\n",
    "                failed += 1\n",
    "            print(f\"[{completed}/{len(tasks)}] ComplÃ©tÃ©\")\n",
    "    print(f\"\\nâœ… TerminÃ©: {completed-failed}/{completed} succÃ¨s\")\n",
    "\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Period(str, Enum):\n",
    "    DAILY = \"daily\"\n",
    "    MONTHLY = \"monthly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dca37f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'symbol = \\'BTCUSDT\\'\\ndate = datetime(2021, 1, 1)\\nloader = DataLoader(base_url=\"https://data.binance.vision/data/futures/um\", \\n                    period=Period.DAILY.value, target=\"aggTrades\", data_dir=\"./data\")\\ncsv_file = loader.load(symbol=symbol, date=date)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"symbol = 'BTCUSDT'\n",
    "date = datetime(2021, 1, 1)\n",
    "loader = DataLoader(base_url=\"https://data.binance.vision/data/futures/um\", \n",
    "                    period=Period.DAILY.value, target=\"aggTrades\", data_dir=\"./data\")\n",
    "csv_file = loader.load(symbol=symbol, date=date)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76593b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Total: 25 fichiers Ã  tÃ©lÃ©charger\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "def generate_dates(start_date: datetime, end_date: datetime, period: Period) -> List[datetime]:\n",
    "    dates = []\n",
    "    if period == Period.MONTHLY:\n",
    "        year = start_date.year\n",
    "        while year <= end_date.year:\n",
    "            start_month = 1 if year > start_date.year else start_date.month\n",
    "            end_month = 12 if year < end_date.year else end_date.month\n",
    "            \n",
    "            for month in range(start_month, end_month + 1):\n",
    "                dates.append(datetime(year, month, 1))\n",
    "            year += 1\n",
    "    \n",
    "    elif period == Period.DAILY:\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            dates.append(current_date)\n",
    "            current_date += timedelta(days=1)\n",
    "    return dates\n",
    "\n",
    "\n",
    "# Utilisation\n",
    "symbols = ['BTCUSDT']\n",
    "START = datetime(2024, 1, 1)\n",
    "END = datetime.now()\n",
    "dates = generate_dates(start_date=START, end_date=END, period=Period.MONTHLY)\n",
    "print(f\"ðŸ“Š Total: {len(dates)} fichiers Ã  tÃ©lÃ©charger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ DÃ©marrage: 25 tÃ¢ches avec 10 workers\n",
      "\n",
      "data\\BTCUSDT-aggTrades-2024-06.csv\n",
      "â†’ BTCUSDT_5m_2024-06.parquet (8640 barres)\n",
      "[1/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-09.csv\n",
      "â†’ BTCUSDT_5m_2024-09.parquet (8640 barres)\n",
      "[2/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-02.csv\n",
      "â†’ BTCUSDT_5m_2024-02.parquet (8352 barres)\n",
      "[3/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-01.csv\n",
      "â†’ BTCUSDT_5m_2024-01.parquet (8928 barres)\n",
      "[4/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-08.csv\n",
      "â†’ BTCUSDT_5m_2024-08.parquet (8928 barres)\n",
      "[5/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-12.csv\n",
      "â†’ BTCUSDT_5m_2024-12.parquet (8928 barres)\n",
      "[6/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-05.csv\n",
      "â†’ BTCUSDT_5m_2024-05.parquet (8928 barres)\n",
      "[7/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-11.csv\n",
      "â†’ BTCUSDT_5m_2024-11.parquet (8640 barres)\n",
      "[8/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-07.csv\n",
      "â†’ BTCUSDT_5m_2024-07.parquet (8928 barres)\n",
      "[9/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-10.csv\n",
      "â†’ BTCUSDT_5m_2024-10.parquet (8926 barres)\n",
      "[10/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2024-04.csv\n",
      "â†’ BTCUSDT_5m_2024-04.parquet (8640 barres)\n",
      "[11/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2025-02.csv\n",
      "â†’ BTCUSDT_5m_2025-02.parquet (8064 barres)\n",
      "[12/25] ComplÃ©tÃ©\n",
      "data\\BTCUSDT-aggTrades-2025-04.csv\n",
      "data\\BTCUSDT-aggTrades-2025-08.csv\n",
      "data\\BTCUSDT-aggTrades-2024-03.csv\n",
      "data\\BTCUSDT-aggTrades-2025-05.csv\n"
     ]
    }
   ],
   "source": [
    "run_parallel(symbols, dates,\n",
    "             period=Period.MONTHLY.value,\n",
    "             data_dir=\"./data\", \n",
    "             max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "def concat_tick_data(symbol: str, interval: str, years: list = [2021, 2025], \n",
    "                     save: bool = True, output_dir: str = \"data\") -> pl.DataFrame:\n",
    "    dataframes = []\n",
    "    for year in years:\n",
    "        for month in range(1, 13):\n",
    "            for day in range(1, 32):\n",
    "                filename = f\"{symbol}_{interval}_{year}-{month:02d}-{day:02d}.parquet\"\n",
    "                file = os.path.join(output_dir, filename)\n",
    "                try:\n",
    "                    df = pl.read_parquet(file)\n",
    "                    if df.shape[0] < 288:\n",
    "                        print(f\" shape error : {filename} - {df.shape[0]}\")\n",
    "                    dataframes.append(df)\n",
    "                except:\n",
    "                    print(filename)\n",
    "                    pass\n",
    "        \n",
    "    if dataframes:\n",
    "        data = pl.concat(dataframes)\n",
    "        print(f\"{symbol}_{interval}: {len(data):,} rows\")\n",
    "        \n",
    "        if save:\n",
    "            output_file = os.path.join(output_dir, f\"{symbol}_{interval}_combined.parquet\")\n",
    "            data.write_parquet(output_file)\n",
    "        \n",
    "        return data\n",
    "    else:\n",
    "        return pl.dataframe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = {}\n",
    "symbols_gaps = {}\n",
    "SYMBOLS = ['BTCUSDT', 'ETHUSDT', 'TRXUSDT']\n",
    "SYMBOLS = ['BTCUSDT']\n",
    "for symbol in SYMBOLS:\n",
    "    df = concat_tick_data(symbol, \"5m\")\n",
    "    gaps = df.select(['timestamp',\n",
    "                      pl.col('timestamp').diff().alias('time_diff')\n",
    "                      ]).filter(\n",
    "                          pl.col('time_diff') > pl.duration(minutes=5)\n",
    "                          )\n",
    "    \n",
    "    symbols_gaps[symbol] = gaps['timestamp'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85945502",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(\"data/BTCUSDT_5m_2025-11-15.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csyst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
